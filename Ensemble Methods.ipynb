{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "atmospheric-handy",
   "metadata": {},
   "source": [
    "For imbalanced datasets, we have seen that we can use methods at the data level (Under-sampling and Over-sampling), cost-sensitive methods like higher miss-classification costs (where we add a cost in the cost function of the algorithms) and also ensemble algorithms, like bagging and boosting where we construct multiple learners from the data and we aggregate their predictions (but they usually optimize the global accuracy, not the imbalance class).\n",
    "\n",
    "For imbalanced datasets, what has been done is to combine ensemble algorithms, like boosting and bagging, to a data level methods (like oversampling) or a cost-sensitive method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-catalog",
   "metadata": {},
   "source": [
    "The objective of Ensemble Learning is to improve model performance, and the idea is to combine several classifiers whose combination outperforms every individual counterpart. To do that, several classifiers are built from the data, and their decisions combined or aggregated.\n",
    "\n",
    "To classify a new example, it is submitted to all the classifiers, and the prediction of all the classifiers are considered to make the final deicision, wether it is a Hard voting (majority of votes) or a Soft Voting (high % of belonging to one class).\n",
    "\n",
    "The motivation to combine classifiers is to improve their generalization, because each classifier is know to make errors, but beause all the classifiers make different kind of errors, the missclassified examples will not be the same. Therefore, they are complimentary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-northwest",
   "metadata": {},
   "source": [
    "Bagging consists on creating different datasets by bootstrapping (creating data with samples from the data) with replacement the original data, then train a classifier on each boostramp sample and then combine the predictions either by average or majority vote. On a randomForest, we use decision tree to make the predictions where the number of features corresponds to a subsets of features (sqrt(n) usually). So to sum up, classifiers are built in parallel, and trained on subsamples of data, where every classifier has a similar weight towards the final prediction.\n",
    "\n",
    "The injected randomness creates de-correlated, or de-couples classifiers which combined improves the overall generalization.\n",
    "\n",
    "Off-the-shelf algorithms optimize the accuracy, which is not suitable for imbalanced datasets. We can use under or over-sampling during boostrap to create balanced datasets.\n",
    "\n",
    "Examples:\n",
    "* UnderBagging: Random Under-Sampling + Bagging\n",
    "* Balanced RandomForests: Random Under-Samplin + Bagging of Decision Trees\n",
    "* OverBagging: Random Over-Sampling (final balacing ratio = 1) + Bagging\n",
    "* SMOTEBagging: SMOTE (majority class boostrapped with replacement and minority class a % is boostrapped with replacement from 10-100% on each iteration and the rest is created by smote until desired balacing ratio is reached)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-wallet",
   "metadata": {},
   "source": [
    "Boosting constists on building classifiers in a sequential fashion. Classifiers are trained on all data, and observations are given weights which reflect how difficult they are to classify. Each classifier's prediction has a different weight towards final decision, based on their accuracy.\n",
    "\n",
    "For the first classifier, all the observations are given the same weight, then the second one, all the observations miss-classified by previous classifier have a higher weigth then the weights are adjusted in each iteration, that is for each new classifier. The classifiers also get a weight based on their accuracy, that weights their overall contribution to the final prediction.\n",
    "\n",
    "AdaBoost: G(x) = sign(sum(alpha(m) * Gm(x))) where Gm(x) corresponds to each classifier and alpha(m) is the weight towards the final outcome.\n",
    "\n",
    "Generalized additive model: f(x) = sum(B(m)b(X;gamma(m))) where b(x,gamma(m)) corresponds to each classifier, for trees gamma(m) refers to tree splits and B(m) is its weight towards the final outcome. \n",
    "\n",
    "So at each iteration, each new classifier minimisez the difference between its predicitions and the residuals of the previous classifier.\n",
    "\n",
    "Example of Boosting methods and under/over-sampling methods:\n",
    "* RUSBoost: Random Under-Sampling + Boosting, where it provides a faster training because it reduces the size of the dataset, and the lost of information is compensated in subsequent iterations. It works as follow:\n",
    "    * Get the dataset\n",
    "    * Random Under-Sampler\n",
    "    * Train a classifier on under-sampled data\n",
    "    * Calculate error on over entire data\n",
    "    * Adjust weights for all observations\n",
    "    * Repeat\n",
    "    \n",
    "* SMOTEBOOST: SMOTEBoost, which is a combinaison of SMOTE + Boosting, which ceates more instances of the minority class, that adds diversity and improves classifier accuracy, but in exchange requires more computational time to perform SMOTE at each iteration. It works as follow:\n",
    "    * Get the dataset\n",
    "    * SMOTE (synthetic samples weight = 1 / N(obs)) with all the weights normalised to sum 1\n",
    "    * Train a classifier on SMOTE data\n",
    "    * Calculate error on over entire data\n",
    "    * Adjust weights for all observations\n",
    "    * Repeat\n",
    "    \n",
    "* RAMOBoost: ADASYN (adaptation) + Boosting, which resample more Xmin with more neighbours from Xmaj and create more synthetic examples from more Xmin with more neighbours from Xmaj. It is computationally expensive and works as follow:\n",
    "    * Get the dataset\n",
    "    * ADASYN Based (examples closer to the boundaries between the class)\n",
    "    * Train a classifier on ADASYN data\n",
    "    * Calculate error on over entire data\n",
    "    * Adjust weights for all observations\n",
    "    * Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-vocabulary",
   "metadata": {},
   "source": [
    "We can of course combine Bagging + Boosting + Re-Sampling together, with bagging (various classifiers) that are boosting from bags of data that are create with random under-sampling. This is called an ensemble of ensembles.\n",
    "\n",
    "We can create models like EasyEnsemble which is a Random Under-Sampling + bagging of AdaBoost or BalanceCascada (iterative instead of parallel) where all the classifiers created one after another where the correctly classified observations from the majority class are removed at each iterations ( Subset data by RUS -> Train AdaBoost -> Remove observations from the majority class that are correctly classified from original data -> Repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-tours",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
